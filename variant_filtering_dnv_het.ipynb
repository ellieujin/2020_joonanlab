{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/titan2/UDP_SV'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 09:25:16 Hail: WARN: This Hail JAR was compiled for Spark 2.4.5, running with Spark 2.4.1.\n",
      "  Compatibility is not guaranteed.\n",
      "2020-07-27 09:25:16 Hail: INFO: SparkUI: http://163.152.180.157:4040\n",
      "Running on Apache Spark version 2.4.1\n",
      "SparkUI available at http://163.152.180.157:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.49-11ae8408bad0\n",
      "LOGGING: writing to /home/titan/Hail/UDP/hail-20200727-0925-0.2.49-11ae8408bad0.log\n"
     ]
    }
   ],
   "source": [
    "hl.init(tmp_dir=\"/titan2/UDP_SV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mt= hl.import_vcf('/titan2/UDP_SV/temp_dnv_het_3_3.vcf', reference_genome='GRCh38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Allele', 'Consequence', 'IMPACT', 'SYMBOL', 'Gene', 'Feature_type', 'Feature', 'BIOTYPE', 'EXON', 'INTRON', 'HGVSc', 'HGVSp', 'cDNA_position', 'CDS_position', 'Protein_position', 'Amino_acids', 'Codons', 'Existing_variation', 'DISTANCE', 'STRAND', 'FLAGS', 'VARIANT_CLASS', 'SYMBOL_SOURCE', 'HGNC_ID', 'CANONICAL', 'MANE', 'TSL', 'APPRIS', 'CCDS', 'ENSP', 'SWISSPROT', 'TREMBL', 'UNIPARC', 'SOURCE', 'GENE_PHENO', 'NEAREST', 'SIFT', 'PolyPhen', 'DOMAINS', 'miRNA', 'HGVS_OFFSET', 'AF', 'AFR_AF', 'AMR_AF', 'EAS_AF', 'EUR_AF', 'SAS_AF', 'AA_AF', 'EA_AF', 'gnomAD_AF', 'gnomAD_AFR_AF', 'gnomAD_AMR_AF', 'gnomAD_ASJ_AF', 'gnomAD_EAS_AF', 'gnomAD_FIN_AF', 'gnomAD_NFE_AF', 'gnomAD_OTH_AF', 'gnomAD_SAS_AF', 'MAX_AF', 'MAX_AF_POPS', 'CLIN_SIG', 'SOMATIC', 'PHENO', 'PUBMED', 'MOTIF_NAME', 'MOTIF_POS', 'HIGH_INF_POS', 'MOTIF_SCORE_CHANGE', 'SV_overlap_AF', 'SV_overlap_PC', 'SV_overlap_name', 'gnomADg', 'gnomADg_AF']\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with open('/titan2/UDP_SV/temp_dnv_het_3_3.vcf', 'rt') as f:\n",
    "    for l in f:\n",
    "        if 'ID=CSQ' in l:\n",
    "            temp = l.strip('\\n').split('|')\n",
    "            break\n",
    "\n",
    "temp[0] = 'Allele';temp[72] = 'gnomADg_AF'\n",
    "\n",
    "print(temp)\n",
    "print(len(temp))\n",
    "#print(temp.index('Consequence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Variant filtering\n",
    "> - `filter == PASS`인 variant만 남기기\n",
    "> - multi allelic 확인 및 제외\n",
    "> - LCR(low complexity region) 제외\n",
    "> - gnomad filtering\n",
    "> - ~AC ==1/2(hom)~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  `filter == PASS`인 variant만 남기기 \n",
    "mt = mt.filter_rows(hl.len(mt.filters) == 0)\n",
    "\n",
    "## multi allelic 제외\n",
    "mt = hl.split_multi(mt)\n",
    "mt = mt.filter_rows(mt.was_split == False)\n",
    "\n",
    "\n",
    "## lcr(low complexity region)제외\n",
    "#lcr_bed = hl.import_bed('../../Resources/lcr/LCR-hs38.bed', reference_genome = 'GRCh38')\n",
    "#mt = mt.filter_rows(~hl.is_defined(lcr_bed[mt.locus]))\n",
    "\n",
    "## gnomad filtering\n",
    "mt = mt.annotate_rows(csq = mt.info.CSQ)\n",
    "mt = mt.transmute_rows(csq_gnomADg_AF = mt.csq.map(lambda x: x.split('\\|')[72]))\n",
    "l = hl.array(['0',''])\n",
    "mt = mt.filter_rows(mt.csq_gnomADg_AF.all(lambda x: l.contains(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 09:01:08 Hail: INFO: Coerced sorted dataset\n",
      "2020-07-27 09:01:08 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tb = mt.entries()\n",
    "tb = tb.key_by(tb.locus,tb.alleles,tb.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export data + CSQ annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(CSQ 전체)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSQ(table):\n",
    "    t = table\n",
    "    t = t.annotate(v1 = hl.tuple([t.locus.contig.replace(\"chr\", \"\"),hl.str(t.locus.position)]),\n",
    "                   v2 = hl.tuple([t.alleles[0],t.alleles[1]]))\n",
    "    t = t.transmute(variant = hl.delimit(hl.array([t.v1[0],t.v1[1],t.v2[0],t.v2[1]]), \":\"))\n",
    "    t = t.annotate(CSQ= t.info.CSQ).explode('CSQ')\n",
    "    t = t.transmute(csq = t.CSQ.split('\\|')).explode('csq')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = CSQ(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 09:01:14 Hail: INFO: Coerced sorted dataset\n",
      "2020-07-27 09:01:15 Hail: INFO: Coerced sorted dataset\n",
      "2020-07-27 09:01:15 Hail: INFO: Coerced sorted dataset\n",
      "2020-07-27 09:01:21 Hail: INFO: merging 2 files totalling 762.3M...\n",
      "2020-07-27 09:01:22 Hail: INFO: while writing:\n",
      "    /titan2/UDP_SV/df_csq_dnv_het_3_3.tsv\n",
      "  merge time: 1.080s\n"
     ]
    }
   ],
   "source": [
    "tb.export(\"/titan2/UDP_SV/df_csq_dnv_het_3_3.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/titan2/UDP_SV'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mt= hl.import_vcf('/titan2/UDP_SV/temp_dnv_het_3_2.vcf', reference_genome='GRCh38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Allele', 'Consequence', 'IMPACT', 'SYMBOL', 'Gene', 'Feature_type', 'Feature', 'BIOTYPE', 'EXON', 'INTRON', 'HGVSc', 'HGVSp', 'cDNA_position', 'CDS_position', 'Protein_position', 'Amino_acids', 'Codons', 'Existing_variation', 'DISTANCE', 'STRAND', 'FLAGS', 'VARIANT_CLASS', 'SYMBOL_SOURCE', 'HGNC_ID', 'CANONICAL', 'MANE', 'TSL', 'APPRIS', 'CCDS', 'ENSP', 'SWISSPROT', 'TREMBL', 'UNIPARC', 'SOURCE', 'GENE_PHENO', 'NEAREST', 'SIFT', 'PolyPhen', 'DOMAINS', 'miRNA', 'HGVS_OFFSET', 'AF', 'AFR_AF', 'AMR_AF', 'EAS_AF', 'EUR_AF', 'SAS_AF', 'AA_AF', 'EA_AF', 'gnomAD_AF', 'gnomAD_AFR_AF', 'gnomAD_AMR_AF', 'gnomAD_ASJ_AF', 'gnomAD_EAS_AF', 'gnomAD_FIN_AF', 'gnomAD_NFE_AF', 'gnomAD_OTH_AF', 'gnomAD_SAS_AF', 'MAX_AF', 'MAX_AF_POPS', 'CLIN_SIG', 'SOMATIC', 'PHENO', 'PUBMED', 'MOTIF_NAME', 'MOTIF_POS', 'HIGH_INF_POS', 'MOTIF_SCORE_CHANGE', 'SV_overlap_AF', 'SV_overlap_PC', 'SV_overlap_name', 'gnomADg', 'gnomADg_AF']\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with open('/titan2/UDP_SV/temp_dnv_het_3_2.vcf', 'rt') as f:\n",
    "    for l in f:\n",
    "        if 'ID=CSQ' in l:\n",
    "            temp = l.strip('\\n').split('|')\n",
    "            break\n",
    "\n",
    "temp[0] = 'Allele';temp[72] = 'gnomADg_AF'\n",
    "\n",
    "print(temp)\n",
    "print(len(temp))\n",
    "#print(temp.index('Consequence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Variant filtering\n",
    "> - `filter == PASS`인 variant만 남기기\n",
    "> - multi allelic 확인 및 제외\n",
    "> - LCR(low complexity region) 제외\n",
    "> - gnomad filtering\n",
    "> - ~AC ==1/2(hom)~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  `filter == PASS`인 variant만 남기기 \n",
    "mt = mt.filter_rows(hl.len(mt.filters) == 0)\n",
    "\n",
    "## multi allelic 제외\n",
    "mt = hl.split_multi(mt)\n",
    "mt = mt.filter_rows(mt.was_split == False)\n",
    "\n",
    "\n",
    "## lcr(low complexity region)제외\n",
    "#lcr_bed = hl.import_bed('../../Resources/lcr/LCR-hs38.bed', reference_genome = 'GRCh38')\n",
    "#mt = mt.filter_rows(~hl.is_defined(lcr_bed[mt.locus]))\n",
    "\n",
    "## gnomad filtering\n",
    "mt = mt.annotate_rows(csq = mt.info.CSQ)\n",
    "mt = mt.transmute_rows(csq_gnomADg_AF = mt.csq.map(lambda x: x.split('\\|')[72]))\n",
    "l = hl.array(['0',''])\n",
    "mt = mt.filter_rows(mt.csq_gnomADg_AF.all(lambda x: l.contains(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 09:31:19 Hail: INFO: Coerced sorted dataset\n",
      "2020-07-27 09:31:19 Hail: INFO: Coerced sorted dataset\n",
      "2020-07-27 09:31:20 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><thead><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;\"></div></td></tr><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">locus</div></td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \" colspan=\"1\"><div style=\"text-align: left;border-bottom: solid 2px #000; padding-bottom: 5px\">alleles</div></td></tr><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">locus&lt;GRCh38&gt;</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; text-align: left;\">array&lt;str&gt;</td></tr>\n",
       "</thead><tbody><tr><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">chr19:34983288</td><td style=\"white-space: nowrap; max-width: 500px; overflow: hidden; text-overflow: ellipsis; \">[&quot;T&quot;,&quot;&lt;DUP:TANDEM&gt;&quot;]</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing the first 0 of 1 columns</p>\n"
      ],
      "text/plain": [
       "+----------------+----------------------+\n",
       "| locus          | alleles              |\n",
       "+----------------+----------------------+\n",
       "| locus<GRCh38>  | array<str>           |\n",
       "+----------------+----------------------+\n",
       "| chr19:34983288 | [\"T\",\"<DUP:TANDEM>\"] |\n",
       "+----------------+----------------------+\n",
       "showing the first 0 of 1 columns"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 09:31:29 Hail: INFO: Coerced sorted dataset\n",
      "2020-07-27 09:31:29 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "ename": "FatalError",
     "evalue": "SparkException: Failed to get broadcast_10_piece0 of broadcast_10\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 1 times, most recent failure: Lost task 1.0 in stage 11.0 (TID 13, localhost, executor driver): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_10_piece0 of broadcast_10\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:207)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat is.hail.backend.spark.SparkBroadcastValue.value(SparkBackend.scala:41)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:693)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:672)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:64)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:62)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:602)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:601)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anon$1.hasNext(RichContextRDDRegionValue.scala:188)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:730)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:728)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anon$1.hasNext(RichContextRDD.scala:31)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212)\n\tat scala.collection.AbstractIterator.fold(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to get broadcast_10_piece0 of broadcast_10\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:179)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:151)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:231)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:211)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)\n\t... 50 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat is.hail.rvd.RVD.count(RVD.scala:734)\n\tat is.hail.expr.ir.Interpret$$anonfun$run$3.apply$mcJ$sp(Interpret.scala:801)\n\tat is.hail.expr.ir.Interpret$$anonfun$run$3.apply(Interpret.scala:801)\n\tat is.hail.expr.ir.Interpret$$anonfun$run$3.apply(Interpret.scala:801)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat is.hail.expr.ir.Interpret$.run(Interpret.scala:801)\n\tat is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16)\n\tat is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$$anonfun$1.apply(InterpretNonCompilable.scala:25)\n\tat is.hail.expr.ir.InterpretNonCompilable$$anonfun$1.apply(InterpretNonCompilable.scala:25)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat is.hail.expr.ir.InterpretNonCompilable$.rewriteChildren$1(InterpretNonCompilable.scala:25)\n\tat is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:54)\n\tat is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:56)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:51)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)\n\tat is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18)\n\tat is.hail.utils.package$.using(package.scala:602)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:230)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:304)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:324)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\njava.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_10_piece0 of broadcast_10\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:207)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat is.hail.backend.spark.SparkBroadcastValue.value(SparkBackend.scala:41)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:693)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:672)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:64)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:62)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:602)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:601)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anon$1.hasNext(RichContextRDDRegionValue.scala:188)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:730)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:728)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anon$1.hasNext(RichContextRDD.scala:31)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212)\n\tat scala.collection.AbstractIterator.fold(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\norg.apache.spark.SparkException: Failed to get broadcast_10_piece0 of broadcast_10\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:179)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:151)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:231)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:211)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:207)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat is.hail.backend.spark.SparkBroadcastValue.value(SparkBackend.scala:41)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:693)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:672)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:64)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:62)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:602)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:601)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anon$1.hasNext(RichContextRDDRegionValue.scala:188)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:730)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:728)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anon$1.hasNext(RichContextRDD.scala:31)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212)\n\tat scala.collection.AbstractIterator.fold(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n\n\nHail version: 0.2.49-11ae8408bad0\nError summary: SparkException: Failed to get broadcast_10_piece0 of broadcast_10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-8377e223e0ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/hail/lib/python3.7/site-packages/hail/matrixtable.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2424\u001b[0m         \"\"\"\n\u001b[1;32m   2425\u001b[0m         \u001b[0mcount_ir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMatrixCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_ir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m     @typecheck_method(output=str,\n",
      "\u001b[0;32m~/anaconda3/envs/hail/lib/python3.7/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mjir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_value_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mtimings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hail/lib/python3.7/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n\u001b[1;32m     40\u001b[0m                              \u001b[0;34m'Hail version: %s\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                              'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n",
      "\u001b[0;31mFatalError\u001b[0m: SparkException: Failed to get broadcast_10_piece0 of broadcast_10\n\nJava stack trace:\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 1 times, most recent failure: Lost task 1.0 in stage 11.0 (TID 13, localhost, executor driver): java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_10_piece0 of broadcast_10\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:207)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat is.hail.backend.spark.SparkBroadcastValue.value(SparkBackend.scala:41)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:693)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:672)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:64)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:62)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:602)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:601)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anon$1.hasNext(RichContextRDDRegionValue.scala:188)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:730)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:728)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anon$1.hasNext(RichContextRDD.scala:31)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212)\n\tat scala.collection.AbstractIterator.fold(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to get broadcast_10_piece0 of broadcast_10\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:179)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:151)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:231)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:211)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)\n\t... 50 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1092)\n\tat is.hail.rvd.RVD.count(RVD.scala:734)\n\tat is.hail.expr.ir.Interpret$$anonfun$run$3.apply$mcJ$sp(Interpret.scala:801)\n\tat is.hail.expr.ir.Interpret$$anonfun$run$3.apply(Interpret.scala:801)\n\tat is.hail.expr.ir.Interpret$$anonfun$run$3.apply(Interpret.scala:801)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat is.hail.expr.ir.Interpret$.run(Interpret.scala:801)\n\tat is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16)\n\tat is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$$anonfun$1.apply(InterpretNonCompilable.scala:25)\n\tat is.hail.expr.ir.InterpretNonCompilable$$anonfun$1.apply(InterpretNonCompilable.scala:25)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat is.hail.expr.ir.InterpretNonCompilable$.rewriteChildren$1(InterpretNonCompilable.scala:25)\n\tat is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:54)\n\tat is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:56)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:51)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)\n\tat is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18)\n\tat is.hail.utils.package$.using(package.scala:602)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:230)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:304)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:324)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\njava.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_10_piece0 of broadcast_10\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:207)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat is.hail.backend.spark.SparkBroadcastValue.value(SparkBackend.scala:41)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:693)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:672)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:64)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:62)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:602)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:601)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anon$1.hasNext(RichContextRDDRegionValue.scala:188)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:730)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:728)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anon$1.hasNext(RichContextRDD.scala:31)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212)\n\tat scala.collection.AbstractIterator.fold(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\norg.apache.spark.SparkException: Failed to get broadcast_10_piece0 of broadcast_10\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:179)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:151)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:151)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:231)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:211)\n\tat org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)\n\tat org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:207)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)\n\tat org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)\n\tat org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)\n\tat is.hail.backend.spark.SparkBroadcastValue.value(SparkBackend.scala:41)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:693)\n\tat is.hail.expr.ir.EmitClassBuilder$$anon$1.apply(EmitClassBuilder.scala:672)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:64)\n\tat is.hail.expr.ir.TableValue$$anonfun$2.apply(TableValue.scala:62)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:602)\n\tat is.hail.rvd.RVD$$anonfun$17.apply(RVD.scala:601)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitionsWithContextAndIndex$1$$anonfun$apply$13$$anonfun$apply$14.apply(ContextRDD.scala:221)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anonfun$5.apply(RichContextRDDRegionValue.scala:179)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.io.RichContextRDDLong$$anonfun$boundary$extension$2$$anon$1.hasNext(RichContextRDDRegionValue.scala:188)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:730)\n\tat is.hail.rvd.RVD$$anonfun$23.apply(RVD.scala:728)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.sparkextras.ContextRDD$$anonfun$cmapPartitions$1$$anonfun$apply$9.apply(ContextRDD.scala:208)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anonfun$1.apply(RichContextRDD.scala:22)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441)\n\tat is.hail.utils.richUtils.RichContextRDD$$anonfun$cleanupRegions$1$$anon$1.hasNext(RichContextRDD.scala:31)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.fold(TraversableOnce.scala:212)\n\tat scala.collection.AbstractIterator.fold(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1$$anonfun$20.apply(RDD.scala:1096)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n\n\nHail version: 0.2.49-11ae8408bad0\nError summary: SparkException: Failed to get broadcast_10_piece0 of broadcast_10"
     ]
    }
   ],
   "source": [
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 09:32:30 Hail: WARN: entries(): Resulting entries table is sorted by '(row_key, col_key)'.\n",
      "    To preserve row-major matrix table order, first unkey columns with 'key_cols_by()'\n"
     ]
    }
   ],
   "source": [
    "tb = mt.entries()\n",
    "tb = tb.key_by(tb.locus,tb.alleles,tb.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export data + CSQ annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(CSQ 전체)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSQ(table):\n",
    "    t = table\n",
    "    t = t.annotate(v1 = hl.tuple([t.locus.contig.replace(\"chr\", \"\"),hl.str(t.locus.position)]),\n",
    "                   v2 = hl.tuple([t.alleles[0],t.alleles[1]]))\n",
    "    t = t.transmute(variant = hl.delimit(hl.array([t.v1[0],t.v1[1],t.v2[0],t.v2[1]]), \":\"))\n",
    "    t = t.annotate(CSQ= t.info.CSQ).explode('CSQ')\n",
    "    t = t.transmute(csq = t.CSQ.split('\\|')).explode('csq')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = CSQ(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-27 09:32:39 Hail: INFO: Coerced sorted dataset\n",
      "2020-07-27 09:32:39 Hail: INFO: Coerced sorted dataset\n",
      "2020-07-27 17:58:26 Hail: INFO: merging 1 files totalling 2.5T...\n"
     ]
    },
    {
     "ename": "FatalError",
     "evalue": "IOException: No space left on device\n\nJava stack trace:\norg.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\tat org.apache.hadoop.fs.FSOutputSummer.flush(FSOutputSummer.java:182)\n\tat java.io.FilterOutputStream.flush(FilterOutputStream.java:140)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat is.hail.io.fs.HadoopFS$$anon$1.flush(HadoopFS.scala:35)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\tat is.hail.utils.package$.using(package.scala:622)\n\tat is.hail.io.fs.FS$class.copyMergeList(FS.scala:289)\n\tat is.hail.io.fs.HadoopFS.copyMergeList(HadoopFS.scala:70)\n\tat is.hail.io.fs.FS$$anonfun$1.apply$mcV$sp(FS.scala:269)\n\tat is.hail.io.fs.FS$$anonfun$1.apply(FS.scala:269)\n\tat is.hail.io.fs.FS$$anonfun$1.apply(FS.scala:269)\n\tat is.hail.utils.package$.time(package.scala:151)\n\tat is.hail.io.fs.FS$class.copyMerge(FS.scala:268)\n\tat is.hail.io.fs.HadoopFS.copyMerge(HadoopFS.scala:70)\n\tat is.hail.utils.richUtils.RichRDD$.writeTable$extension(RichRDD.scala:117)\n\tat is.hail.expr.ir.TableValue.export(TableValue.scala:98)\n\tat is.hail.expr.ir.TableTextWriter.apply(TableWriter.scala:337)\n\tat is.hail.expr.ir.Interpret$.run(Interpret.scala:811)\n\tat is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16)\n\tat is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:56)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:51)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)\n\tat is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18)\n\tat is.hail.utils.package$.using(package.scala:602)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:230)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:304)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:324)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\njava.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\tat org.apache.hadoop.fs.FSOutputSummer.flush(FSOutputSummer.java:182)\n\tat java.io.FilterOutputStream.flush(FilterOutputStream.java:140)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat is.hail.io.fs.HadoopFS$$anon$1.flush(HadoopFS.scala:35)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\tat is.hail.utils.package$.using(package.scala:622)\n\tat is.hail.io.fs.FS$class.copyMergeList(FS.scala:289)\n\tat is.hail.io.fs.HadoopFS.copyMergeList(HadoopFS.scala:70)\n\tat is.hail.io.fs.FS$$anonfun$1.apply$mcV$sp(FS.scala:269)\n\tat is.hail.io.fs.FS$$anonfun$1.apply(FS.scala:269)\n\tat is.hail.io.fs.FS$$anonfun$1.apply(FS.scala:269)\n\tat is.hail.utils.package$.time(package.scala:151)\n\tat is.hail.io.fs.FS$class.copyMerge(FS.scala:268)\n\tat is.hail.io.fs.HadoopFS.copyMerge(HadoopFS.scala:70)\n\tat is.hail.utils.richUtils.RichRDD$.writeTable$extension(RichRDD.scala:117)\n\tat is.hail.expr.ir.TableValue.export(TableValue.scala:98)\n\tat is.hail.expr.ir.TableTextWriter.apply(TableWriter.scala:337)\n\tat is.hail.expr.ir.Interpret$.run(Interpret.scala:811)\n\tat is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16)\n\tat is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:56)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:51)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)\n\tat is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18)\n\tat is.hail.utils.package$.using(package.scala:602)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:230)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:304)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:324)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n\nHail version: 0.2.49-11ae8408bad0\nError summary: IOException: No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-94642b6752c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/titan2/UDP_SV/df_csq_dnv_het_3_2.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<decorator-gen-1083>\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, output, types_file, header, parallel, delimiter)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hail/lib/python3.7/site-packages/hail/typecheck/check.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(__original_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__original_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__original_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hail/lib/python3.7/site-packages/hail/table.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, output, types_file, header, parallel, delimiter)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExportType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         Env.backend().execute(\n\u001b[0;32m-> 1038\u001b[0;31m             ir.TableWrite(self._tir, ir.TableTextWriter(output, types_file, header, parallel, delimiter)))\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mexprs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnamed_exprs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'GroupedTable'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hail/lib/python3.7/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, ir, timed)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mjir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_value_ir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# print(self._hail_package.expr.ir.Pretty.apply(jir, True, -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jhc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuteJSON\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mtimings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hail/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hail/lib/python3.7/site-packages/hail/backend/spark_backend.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n\u001b[1;32m     40\u001b[0m                              \u001b[0;34m'Hail version: %s\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                              'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             raise FatalError('%s\\n\\nJava stack trace:\\n%s\\n'\n",
      "\u001b[0;31mFatalError\u001b[0m: IOException: No space left on device\n\nJava stack trace:\norg.apache.hadoop.fs.FSError: java.io.IOException: No space left on device\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:248)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\tat org.apache.hadoop.fs.FSOutputSummer.flush(FSOutputSummer.java:182)\n\tat java.io.FilterOutputStream.flush(FilterOutputStream.java:140)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat is.hail.io.fs.HadoopFS$$anon$1.flush(HadoopFS.scala:35)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\tat is.hail.utils.package$.using(package.scala:622)\n\tat is.hail.io.fs.FS$class.copyMergeList(FS.scala:289)\n\tat is.hail.io.fs.HadoopFS.copyMergeList(HadoopFS.scala:70)\n\tat is.hail.io.fs.FS$$anonfun$1.apply$mcV$sp(FS.scala:269)\n\tat is.hail.io.fs.FS$$anonfun$1.apply(FS.scala:269)\n\tat is.hail.io.fs.FS$$anonfun$1.apply(FS.scala:269)\n\tat is.hail.utils.package$.time(package.scala:151)\n\tat is.hail.io.fs.FS$class.copyMerge(FS.scala:268)\n\tat is.hail.io.fs.HadoopFS.copyMerge(HadoopFS.scala:70)\n\tat is.hail.utils.richUtils.RichRDD$.writeTable$extension(RichRDD.scala:117)\n\tat is.hail.expr.ir.TableValue.export(TableValue.scala:98)\n\tat is.hail.expr.ir.TableTextWriter.apply(TableWriter.scala:337)\n\tat is.hail.expr.ir.Interpret$.run(Interpret.scala:811)\n\tat is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16)\n\tat is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:56)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:51)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)\n\tat is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18)\n\tat is.hail.utils.package$.using(package.scala:602)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:230)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:304)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:324)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\njava.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:246)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.writeChunk(ChecksumFileSystem.java:424)\n\tat org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:206)\n\tat org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:163)\n\tat org.apache.hadoop.fs.FSOutputSummer.flush(FSOutputSummer.java:182)\n\tat java.io.FilterOutputStream.flush(FilterOutputStream.java:140)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat is.hail.io.fs.HadoopFS$$anon$1.flush(HadoopFS.scala:35)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\tat is.hail.utils.package$.using(package.scala:622)\n\tat is.hail.io.fs.FS$class.copyMergeList(FS.scala:289)\n\tat is.hail.io.fs.HadoopFS.copyMergeList(HadoopFS.scala:70)\n\tat is.hail.io.fs.FS$$anonfun$1.apply$mcV$sp(FS.scala:269)\n\tat is.hail.io.fs.FS$$anonfun$1.apply(FS.scala:269)\n\tat is.hail.io.fs.FS$$anonfun$1.apply(FS.scala:269)\n\tat is.hail.utils.package$.time(package.scala:151)\n\tat is.hail.io.fs.FS$class.copyMerge(FS.scala:268)\n\tat is.hail.io.fs.HadoopFS.copyMerge(HadoopFS.scala:70)\n\tat is.hail.utils.richUtils.RichRDD$.writeTable$extension(RichRDD.scala:117)\n\tat is.hail.expr.ir.TableValue.export(TableValue.scala:98)\n\tat is.hail.expr.ir.TableTextWriter.apply(TableWriter.scala:337)\n\tat is.hail.expr.ir.Interpret$.run(Interpret.scala:811)\n\tat is.hail.expr.ir.Interpret$.alreadyLowered(Interpret.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.interpretAndCoerce$1(InterpretNonCompilable.scala:16)\n\tat is.hail.expr.ir.InterpretNonCompilable$.is$hail$expr$ir$InterpretNonCompilable$$rewrite$1(InterpretNonCompilable.scala:53)\n\tat is.hail.expr.ir.InterpretNonCompilable$.apply(InterpretNonCompilable.scala:58)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.transform(LoweringPass.scala:56)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3$$anonfun$1.apply(LoweringPass.scala:15)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:15)\n\tat is.hail.expr.ir.lowering.LoweringPass$$anonfun$apply$3.apply(LoweringPass.scala:13)\n\tat is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:69)\n\tat is.hail.expr.ir.lowering.LoweringPass$class.apply(LoweringPass.scala:13)\n\tat is.hail.expr.ir.lowering.InterpretNonCompilablePass$.apply(LoweringPass.scala:51)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:14)\n\tat is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:12)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:12)\n\tat is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)\n\tat is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:318)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:305)\n\tat is.hail.backend.spark.SparkBackend$$anonfun$execute$1.apply(SparkBackend.scala:304)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:20)\n\tat is.hail.expr.ir.ExecuteContext$$anonfun$scoped$1.apply(ExecuteContext.scala:18)\n\tat is.hail.utils.package$.using(package.scala:602)\n\tat is.hail.annotations.Region$.scoped(Region.scala:18)\n\tat is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:18)\n\tat is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:230)\n\tat is.hail.backend.spark.SparkBackend.execute(SparkBackend.scala:304)\n\tat is.hail.backend.spark.SparkBackend.executeJSON(SparkBackend.scala:324)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n\n\n\nHail version: 0.2.49-11ae8408bad0\nError summary: IOException: No space left on device"
     ]
    }
   ],
   "source": [
    "tb.export(\"/titan2/UDP_SV/df_csq_dnv_het_3_2.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "mt= hl.import_vcf('/titan2/UDP_SV/temp_dnv_het_8.vcf', reference_genome='GRCh38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "with open('/titan2/UDP_SV/temp_dnv_het_8.vcf', 'rt') as f:\n",
    "    for l in f:\n",
    "        if 'ID=CSQ' in l:\n",
    "            temp = l.strip('\\n').split('|')\n",
    "            break\n",
    "\n",
    "temp[0] = 'Allele';temp[72] = 'gnomADg_AF'\n",
    "\n",
    "print(temp)\n",
    "print(len(temp))\n",
    "#print(temp.index('Consequence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Variant filtering\n",
    "> - `filter == PASS`인 variant만 남기기\n",
    "> - multi allelic 확인 및 제외\n",
    "> - LCR(low complexity region) 제외\n",
    "> - gnomad filtering\n",
    "> - ~AC ==1/2(hom)~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  `filter == PASS`인 variant만 남기기 \n",
    "mt = mt.filter_rows(hl.len(mt.filters) == 0)\n",
    "\n",
    "## multi allelic 제외\n",
    "mt = hl.split_multi(mt)\n",
    "mt = mt.filter_rows(mt.was_split == False)\n",
    "\n",
    "\n",
    "## lcr(low complexity region)제외\n",
    "#lcr_bed = hl.import_bed('../../Resources/lcr/LCR-hs38.bed', reference_genome = 'GRCh38')\n",
    "#mt = mt.filter_rows(~hl.is_defined(lcr_bed[mt.locus]))\n",
    "\n",
    "## gnomad filtering\n",
    "mt = mt.annotate_rows(csq = mt.info.CSQ)\n",
    "mt = mt.transmute_rows(csq_gnomADg_AF = mt.csq.map(lambda x: x.split('\\|')[72]))\n",
    "l = hl.array(['0',''])\n",
    "mt = mt.filter_rows(mt.csq_gnomADg_AF.all(lambda x: l.contains(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = mt.entries()\n",
    "tb = tb.key_by(tb.locus,tb.alleles,tb.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. HQ het filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export data + CSQ annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(CSQ 전체)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSQ(table):\n",
    "    t = table\n",
    "    t = t.annotate(v1 = hl.tuple([t.locus.contig.replace(\"chr\", \"\"),hl.str(t.locus.position)]),\n",
    "                   v2 = hl.tuple([t.alleles[0],t.alleles[1]]))\n",
    "    t = t.transmute(variant = hl.delimit(hl.array([t.v1[0],t.v1[1],t.v2[0],t.v2[1]]), \":\"))\n",
    "    t = t.annotate(CSQ= t.info.CSQ).explode('CSQ')\n",
    "    t = t.transmute(csq = t.CSQ.split('\\|')).explode('csq')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = CSQ(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.export(\"/titan2/UDP_SV/df_csq_dnv_het_8.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
